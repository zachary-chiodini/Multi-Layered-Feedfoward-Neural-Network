{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size:32px; color:#F4EE00; font-family:cambria\"><i>Multi-Layered, Feedforward Neural Network</i></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#53C8FE; font-family:aparajita; font-size:24px\">\n",
    "    <i>This is a multi-layred, feedforward neural network written from scratch in Python.</i>\n",
    "</span>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"font-size:32px; color:#F4EE00; font-family:cambria\"><i>Importing Libraries</i></h2>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'divide': 'warn', 'over': 'warn', 'under': 'ignore', 'invalid': 'warn'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from time import time\n",
    "from typing import Any, List, Tuple, TypedDict\n",
    "from nptyping import NDArray, Float64\n",
    "np.seterr( over = 'raise' ) # raise error if overflow is encountered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"font-size:32px; color:orange; font-family:cambria\"><i>Defining Data Structures</i></h2>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i style=\"color:#53C8FE; font-family:aparajita; font-size:24px\">Input Matrix</i>\n",
    "<br>\n",
    "<br>\n",
    "<span style=\"font-size:24px\">$\n",
    "X = \\begin{bmatrix}\n",
    "    x_{1,1} & x_{1,2} & \\dots & x_{1,n} \\\\\n",
    "    x_{2,1} & x_{2,2} & \\dots & x_{2,n} \\\\\n",
    "    \\vdots  & \\vdots  & \\ddots & \\vdots \\\\\n",
    "    x_{m,1} & x_{m,2} & \\dots  & x_{m,n}\n",
    "\\end{bmatrix}\n",
    "$</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Number_of_Examples = Any\n",
    "Number_of_Features = Any\n",
    "Input_Matrix = NDArray[ ( Number_of_Examples, Number_of_Features ), Float64 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i style=\"color:#53C8FE; font-family:aparajita; font-size:24px\">Target Matrix</i>\n",
    "<br>\n",
    "<br>\n",
    "<span style=\"font-size:24px\">$\n",
    "Y = \\begin{bmatrix}\n",
    "    y_{1,1} & y_{1,2} & \\dots  & y_{1,k} \\\\\n",
    "    y_{2,1} & y_{2,2} & \\dots  & y_{2,k} \\\\\n",
    "    \\vdots  & \\vdots  & \\ddots & \\vdots \\\\\n",
    "    y_{m,1} & y_{m,2} & \\dots  & y_{m,k}\n",
    "\\end{bmatrix}\n",
    "$</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Number_of_Targets = Any\n",
    "Target_Matrix = NDArray[ ( Number_of_Examples, Number_of_Targets ), Float64 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i style=\"color:#53C8FE; font-family:aparajita; font-size:24px\">Output Matrix</i>\n",
    "<br>\n",
    "<br>\n",
    "<span style=\"font-size:24px\">$\n",
    "\\color{orange}{\\hat{Y}} = \\begin{bmatrix}\n",
    "    \\color{orange}{\\hat{y}_{1,1}} & \\color{orange}{\\hat{y}_{1,2}} & \\dots & \\color{orange}{\\hat{y}_{1,k}} \\\\\n",
    "    \\color{orange}{\\hat{y}_{2,1}} & \\color{orange}{\\hat{y}_{2,2}} & \\dots & \\color{orange}{\\hat{y}_{2,k}} \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    \\color{orange}{\\hat{y}_{m,1}} & \\color{orange}{\\hat{y}_{m,2}} & \\dots & \\color{orange}{\\hat{y}_{m,k}}\n",
    "\\end{bmatrix}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Output_Matrix = NDArray[ ( Number_of_Examples, Number_of_Targets ), Float64 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i style=\"color:#53C8FE; font-family:aparajita; font-size:24px\">Perceptron</i>\n",
    "<br>\n",
    "<br>\n",
    "<span style=\"font-size:24px\">$\n",
    "\\color{green}{P}^{(ℓ)}_{\\color{green}{k}} = \\begin{bmatrix}\n",
    "    \\color{green}{\\omega}^{(ℓ)}_{\\color{green}{1,k}} \\\\\n",
    "    \\color{green}{\\omega}^{(ℓ)}_{\\color{green}{2,k}} \\\\\n",
    "    \\vdots \\\\ \n",
    "    \\color{green}{\\omega}^{(ℓ)}_{\\color{green}{j,k}}\n",
    "\\end{bmatrix} \\\\\n",
    "\\color{green}{b}^{(ℓ)}_{\\color{green}{k}} = \\color{green}{\\omega}^{(ℓ)}_{\\color{green}{0,k}}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Number_of_Inputs = Any\n",
    "Bias, Weight = Float64, Float64\n",
    "Perceptron = NDArray[ ( Number_of_Inputs, 1 ), Weight ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i style=\"color:#53C8FE; font-family:aparajita; font-size:24px\">Layer</i>\n",
    "<br>\n",
    "<br>\n",
    "<span style=\"font-size:24px\">$\n",
    "\\Omega^{(ℓ)} = \\begin{bmatrix}\n",
    "    P^{(ℓ)}_{1} & P^{(ℓ)}_{2} & \\dots & P^{(ℓ)}_{k}\n",
    "\\end{bmatrix} \\\\\n",
    "\\beta^{(ℓ)} = \\begin{bmatrix}\n",
    "    b^{(ℓ)}_{1} & b^{(ℓ)}_{2} & \\dots & b^{(ℓ)}_{k}\n",
    "\\end{bmatrix}\n",
    "$</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Number_of_Perceptrons = Any\n",
    "Weights = NDArray[ ( Number_of_Inputs, Number_of_Perceptrons ), Perceptron ]\n",
    "Biases = NDArray[ ( Number_of_Perceptrons, ), Bias ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i style=\"color:#53C8FE; font-family:aparajita; font-size:24px\">Network</i>\n",
    "<br>\n",
    "<br>\n",
    "<span style=\"font-size:24px\">$\n",
    "\\color{green}{N} = \\begin{bmatrix}\n",
    "    \\begin{bmatrix}\n",
    "       \\color{green}{\\Omega}^{(1)} &\n",
    "       \\color{green}{\\Omega}^{(2)} & \n",
    "       \\dots & \n",
    "       \\color{green}{\\Omega}^{(ℓ)}\n",
    "   \\end{bmatrix} & \n",
    "   \\begin{bmatrix}\n",
    "       \\color{green}{\\beta}^{(1)} &\n",
    "       \\color{green}{\\beta}^{(2)} & \n",
    "       \\dots & \n",
    "       \\color{green}{\\beta}^{(ℓ)}\n",
    "   \\end{bmatrix}\n",
    "\\end{bmatrix}\n",
    "$</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Network_Weights = List[ Weights ]\n",
    "Network_Biases = List[ Biases ]\n",
    "class Network( TypedDict ) :\n",
    "    weights : Network_Weights\n",
    "    biases  : Network_Biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size:32px; color:orange; font-family:cambria\"><i>Creating the Neural Network</i></h1>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#53C8FE; font-family:aparajita\">\n",
    "    <i><span style=\"font-size:24px\">Initialization</span></i>\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNeuralNetwork :\n",
    "    \n",
    "    def __init__( self, perceptrons_per_hidden_layer : List[ int ] = [] ) -> None :\n",
    "        self.score = 0.0\n",
    "        self.perlayer = perceptrons_per_hidden_layer\n",
    "        self.network : Network = { 'weights' : [], 'biases' : [] }\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#53C8FE; font-family:aparajita\">\n",
    "    <i><span style=\"font-size:24px\">Initialize Random Weights and Biases</span></i>\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNeuralNetwork( FeedForwardNeuralNetwork  ) :\n",
    "    \n",
    "    def initialize( self, X : Input_Matrix, Y : Target_Matrix ) -> None :\n",
    "        inputs = X.shape[ 1 ]\n",
    "        self.network = { 'weights' : [], 'biases' : [] }\n",
    "        for perceptrons in self.perlayer :\n",
    "            self.network[ 'weights' ].append( np.random.rand( inputs, perceptrons ) - 0.5 )\n",
    "            self.network[ 'biases' ].append( np.random.rand( perceptrons ) - 0.5 )\n",
    "            inputs = perceptrons\n",
    "        self.network[ 'weights' ].append( np.random.rand( inputs, Y.shape[ 1 ] ) - 0.5 )\n",
    "        self.network[ 'biases' ].append( np.random.rand( Y.shape[ 1 ] ) - 0.5 )\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#53C8FE; font-family:aparajita\">\n",
    "    <i><span style=\"font-size:24px\">Activation Function</span></i>\n",
    "</span>\n",
    "<br>\n",
    "<br>\n",
    "<table align=\"left\">\n",
    "    <tr>\n",
    "        <td style=\"font-size:20px\">$\\color{blue}{Sigmoid}$</td>\n",
    "        <td style=\"font-size:24px; text-align:left\">$\n",
    "        \\color{red}{f}(x) = \\frac{1}{1+\\color{blue}{e}^{-x}}\n",
    "        $</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"font-size:20px\">$\\color{blue}{Derivative}$</td>\n",
    "        <td style=\"font-size:24px\">$\n",
    "        \\color{red}{f}(x) = \\frac{\\color{blue}{e}^{-x}}{(1+\\color{blue}{e}^{-x})^{2}}\n",
    "        $</td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNeuralNetwork( FeedForwardNeuralNetwork  ) :\n",
    "    \n",
    "    def activation( self, x : NDArray[ Float64 ] ) -> NDArray[ Float64 ] :\n",
    "        return 1.0 / ( 1.0 + np.exp( -x ) )\n",
    "    \n",
    "    def derivative( self, x : NDArray[ Float64 ] ) -> NDArray[ Float64 ] :\n",
    "        return np.exp( -x ) / np.square( 1.0 + np.exp( -x ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#53C8FE; font-family:aparajita\">\n",
    "    <i><span style=\"font-size:24px\">Cost and Gradient</span></i>\n",
    "</span>\n",
    "<br>\n",
    "<br>\n",
    "<table align=\"left\">\n",
    "    <tr>\n",
    "        <td style=\"font-size:20px\">$\\color{blue}{Cost\\ Function}$</td>\n",
    "        <td style=\"font-size:24px\">$\n",
    "        \\color{red}{C}(\\color{orange}{Y};\\color{orange}{\\hat{Y}}) = \n",
    "        \\sum_{h=1}^{m}{\\sum_{i=1}^{k} (\\color{orange}{\\hat{y}}_{h,i} - \\color{orange}{y}_{h,i}})^{2} =\n",
    "        \\sum_{h=1}^{m}{\\sum_{i=1}^{k} (\\color{red}{a}^{(ℓ)}_{h,i} - \\color{orange}{y}_{h,i}})^{2} \n",
    "        $</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"font-size:20px\">$\\color{blue}{Gradient}$</td>\n",
    "        <td style=\"font-size:24px; text-align:left\">$\n",
    "        \\nabla^{(ℓ)}_{\\color{red}{a}}\\color{red}{C} = \\begin{bmatrix}\n",
    "                \\frac{\\partial \\color{red}{C}}{\\partial \\color{red}{a}^{(ℓ)}_{\\color{red}{1,1}}} &\n",
    "                \\frac{\\partial \\color{red}{C}}{\\partial \\color{red}{a}^{(ℓ)}_{\\color{red}{1,2}}} &\n",
    "                \\dots & \n",
    "                \\frac{\\partial \\color{red}{C}}{\\partial \\color{red}{a}^{(ℓ)}_{\\color{red}{1,k}}} \n",
    "                \\\\\n",
    "                \\frac{\\partial \\color{red}{C}}{\\partial \\color{red}{a}^{(ℓ)}_{\\color{red}{2,1}}} &\n",
    "                \\frac{\\partial \\color{red}{C}}{\\partial \\color{red}{a}^{(ℓ)}_{\\color{red}{2,2}}} &\n",
    "                \\dots & \n",
    "                \\frac{\\partial \\color{red}{C}}{\\partial \\color{red}{a}^{(ℓ)}_{\\color{red}{2,k}}}\n",
    "                \\\\\n",
    "                \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "                \\\\\n",
    "                \\frac{\\partial \\color{red}{C}}{\\partial \\color{red}{a}^{(ℓ)}_{\\color{red}{m,1}}} &\n",
    "                \\frac{\\partial \\color{red}{C}}{\\partial \\color{red}{a}^{(ℓ)}_{\\color{red}{m,2}}} &\n",
    "                \\dots & \n",
    "                \\frac{\\partial \\color{red}{C}}{\\partial \\color{red}{a}^{(ℓ)}_{\\color{red}{m,k}}}\n",
    "            \\end{bmatrix} = 2(\\color{orange}{\\hat{Y}}-\\color{orange}{Y}) \n",
    "        $</td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNeuralNetwork( FeedForwardNeuralNetwork  ) :\n",
    "    \n",
    "    def cost( self, A : Output_Matrix, Y : Target_Matrix ) -> Float64 :\n",
    "        return np.square( A - Y ).sum()\n",
    "    \n",
    "    def grad( self, A : Output_Matrix, Y : Target_Matrix ) -> NDArray[ Float64 ] :\n",
    "        return 2*( A - Y )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#53C8FE; font-family:aparajita\">\n",
    "    <i><span style=\"font-size:24px\">Forward Propagation</span></i>\n",
    "</span>\n",
    "<br>\n",
    "<br>\n",
    "<table align=\"left\">\n",
    "    <tr>\n",
    "        <td style=\"font-size:20px\">$\\color{blue}{Output\\ and\\ Hidden\\ Layer}$</td>\n",
    "        <td style=\"font-size:24px\">$\n",
    "            \\color{blue}{L}^{(ℓ)}=\n",
    "            \\color{red}{f}⊙(\\color{blue}{L}^{(ℓ-1)}\\color{green}{\\Omega}^{(ℓ)} + \\color{green}{\\beta}^{(ℓ)})\n",
    "            $\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"font-size:20px\">$\\color{blue}{Input Layer}$</td>\n",
    "        <td style=\"font-size:24px; text-align:left\">$\\color{blue}{L}^{(0)} = \\color{blue}{X}$</td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNeuralNetwork( FeedForwardNeuralNetwork  ) :\n",
    "    \n",
    "    def forwardpropagation( self, L : Input_Matrix ) -> Output_Matrix :\n",
    "        for w, b in zip( self.network[ 'weights' ], self.network[ 'biases' ] ) :\n",
    "            L = self.activation( np.matmul( L, w ) + b ) \n",
    "        return L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#53C8FE; font-family:aparajita\">\n",
    "    <i><span style=\"font-size:24px\">Backpropagation</span></i>\n",
    "</span>\n",
    "<br>\n",
    "<br>\n",
    "<table align=\"left\">\n",
    "    <tr>\n",
    "        <td style=\"font-size:20px\">$\\color{blue}{Containers}$</td>\n",
    "        <td style=\"font-size:24px; text-align:left\">\n",
    "            $\n",
    "            \\color{orange}{Z}[ℓ] = \\color{blue}{L}^{(ℓ-1)}\\color{green}{\\Omega}^{(ℓ)}+\\color{green}{\\beta}^{(ℓ)} \\\\\n",
    "            \\color{red}{A}[ℓ] = \\color{blue}{L}^{(ℓ)}\n",
    "            $\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNeuralNetwork( FeedForwardNeuralNetwork  ) :\n",
    "    \n",
    "    def __forwardpropagation( self, X : Input_Matrix, A : List, Z : List ) -> Output_Matrix :\n",
    "        A.append( X ) # len( A ) = len( Z ) + 1\n",
    "        for w, b in zip( self.network[ 'weights' ], self.network[ 'biases' ] ) :\n",
    "            # weighted input to layer\n",
    "            Z.append( np.matmul( A[ -1 ], w ) + b )\n",
    "            # output of layer\n",
    "            A.append( self.activation( Z[ -1 ] ) )\n",
    "        return A[ -1 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#53C8FE; font-family:aparajita; font-size:18px\">\n",
    "    <i><span style=\"font-size:24px\">Backpropagation</span></i>\n",
    "</span>\n",
    "<br>\n",
    "<br>\n",
    "<table align=\"left\">\n",
    "    <tr>\n",
    "        <td style=\"font-size:20px\">$\\color{blue}{Initialization}$</td>\n",
    "        <td style=\"font-size:24px; text-align:left\">\n",
    "            $\n",
    "            \\nabla^{(ℓ)}_{\\color{orange}{z}}\\color{red}{C}=\n",
    "            (\\color{red}{f}'⊙\\color{orange}{Z}[ℓ])⊙\\nabla^{(ℓ)}_{\\color{red}{a}}\\color{red}{C}\n",
    "            $\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"font-size:20px\">$\\color{blue}{Output\\ and\\ Hidden\\ Layer}$</td>\n",
    "        <td style=\"font-size:24px\">\n",
    "            $\n",
    "            \\nabla^{(ℓ)}_{\\color{green}{\\Omega}}\\color{red}{C}=\n",
    "            \\color{red}{A}[ℓ-1]^{T}\\nabla^{(ℓ)}_{\\color{orange}{z}}\\color{red}{C} \\\\\n",
    "            \\nabla^{(ℓ)}_{\\color{green}{\\beta}}\\color{red}{C}=\n",
    "            \\pmb{1}_{1\\times m}\\nabla^{(ℓ)}_{\\color{orange}{z}}\\color{red}{C} \\\\\n",
    "            \\nabla^{(ℓ-1)}_{\\color{orange}{z}}\\color{red}{C}=\n",
    "            (\\color{red}{f}'⊙\\color{orange}{Z}[ℓ-1])⊙\\nabla^{(ℓ)}_{\\color{orange}{z}}\\color{red}{C}(\\color{green}{\\Omega}^{(ℓ)})^{T}\n",
    "            $\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNeuralNetwork( FeedForwardNeuralNetwork  ) :\n",
    "    \n",
    "    def backpropagate( self, grad_z : NDArray[ Float64 ], A : List, Z : List, layer_index : int ) -> Tuple :\n",
    "        # gradient with respect to the weights of the layer\n",
    "        grad_w = np.matmul( A[ layer_index ].T, grad_z ) # len( A ) = len( Z ) + 1\n",
    "        # gradient with respect to the biases of the layer\n",
    "        grad_b = grad_z.sum( axis = 0 )\n",
    "        # gradient with respect to the weighted input of the layer\n",
    "        if layer_index > 0 : # there is no weighted input for layer 0\n",
    "            grad_z = self.derivative( Z[ layer_index - 1 ] ) *\\\n",
    "                     np.matmul( grad_z, self.network[ 'weights' ][ layer_index ].T )\n",
    "        return grad_z, grad_w, grad_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#53C8FE; font-family:aparajita; font-size:24px\">\n",
    "    <i>Gradient Descent</i>\n",
    "</span>\n",
    "<br>\n",
    "<br>\n",
    "<span style=\"font-size:24px\">$\n",
    "\\color{green}{\\Omega}^{(ℓ)} \\rightarrow \\color{green}{\\Omega}^{(ℓ)} - \\frac{r}{m}\\nabla^{(ℓ)}_{\\color{green}{\\Omega}}\\color{red}{C} \\\\\n",
    "\\color{green}{\\beta}^{(ℓ)} \\rightarrow \\color{green}{\\beta}^{(ℓ)} - \\frac{r}{m}\\nabla^{(ℓ)}_{\\color{green}{\\beta}}\\color{red}{C}\n",
    "$</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNeuralNetwork( FeedForwardNeuralNetwork  ) :\n",
    "    \n",
    "    def train( self, X : Input_Matrix, Y : Target_Matrix, \n",
    "               learning_rate = 1.0, convergence = 0.01, \n",
    "               batch_size = 10, max_epoch = 500, max_time = 60 ) -> None :\n",
    "        ''' Stochastic Gradient Descent '''\n",
    "        epoch = 1\n",
    "        start = time()\n",
    "        totgrad = np.inf\n",
    "        self.initialize( X, Y )\n",
    "        output_layer_index = len( self.network[ 'weights' ] ) - 1\n",
    "        while np.sqrt( totgrad ) > convergence :\n",
    "            totgrad = 0\n",
    "            shuffle = np.random.permutation( len( X ) )\n",
    "            X, Y = X[ shuffle ], Y[ shuffle ]\n",
    "            for batch_x, batch_y in zip( \n",
    "                np.array_split( X, len( X ) // batch_size ), \n",
    "                np.array_split( Y, len( Y ) // batch_size ) \n",
    "                ) :\n",
    "                A, Z = [], []\n",
    "                output = self.__forwardpropagation( batch_x, A, Z )\n",
    "                # gradient with respect to the output layer\n",
    "                grad_a = self.grad( output, batch_y )\n",
    "                totgrad += np.linalg.norm( grad_a )**2\n",
    "                # gradient with respect to the weighted input of the output layer\n",
    "                grad_z = self.derivative( Z[ -1 ] ) * grad_a\n",
    "                for layer_index in range( output_layer_index, -1, -1 ) :\n",
    "                    grad_z, grad_w, grad_b = self.backpropagate( grad_z, A, Z, layer_index )\n",
    "                    # updating the weights and biases of layer\n",
    "                    self.network[ 'weights' ][ layer_index ] -= learning_rate * grad_w / len( batch_x )\n",
    "                    self.network[ 'biases' ][ layer_index ] -= learning_rate * grad_b / len( batch_x )\n",
    "            epoch += 1\n",
    "            if time() - start > max_time :\n",
    "                print( 'Maximum runtime encountered.' )\n",
    "                break\n",
    "            if epoch > max_epoch :\n",
    "                print( 'Maximum epoch encountered.' )\n",
    "                break\n",
    "        self.score = self.cost( self.forwardpropagation( X ), Y )\n",
    "        return"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
