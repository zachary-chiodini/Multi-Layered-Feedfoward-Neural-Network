{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size:32px; color:#F4EE00; font-family:cambria\"><i>Multi-Layered, Feedforward Neural Network</i></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#53C8FE; font-family:aparajita; font-size:24px\">\n",
    "    <i>This is a multi-layred, feedforward neural network written from scratch in Python.</i>\n",
    "</span>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"font-size:32px; color:#F4EE00; font-family:cambria\"><i>Importing Libraries</i></h2>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'divide': 'warn', 'over': 'warn', 'under': 'ignore', 'invalid': 'warn'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from time import time\n",
    "from typing import Any, List, Tuple, TypedDict\n",
    "from nptyping import NDArray, Float64\n",
    "np.seterr( over = 'raise' ) # raise error if overflow is encountered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"font-size:32px; color:orange; font-family:cambria\"><i>Defining Data Structures</i></h2>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table width=100%>\n",
    "    <tr><td style=\"font-size:20px; text-align:left\">$Input\\ Matrix$</td></tr>\n",
    "    <tr>\n",
    "        <td style=\"font-size:24px; text-align:left\">\n",
    "            $X = \\begin{bmatrix}\n",
    "                x_{1,1} & x_{1,2} & \\dots & x_{1,n} \\\\\n",
    "                x_{2,1} & x_{2,2} & \\dots & x_{2,n} \\\\\n",
    "                \\vdots  & \\vdots  & \\ddots & \\vdots \\\\\n",
    "                x_{m,1} & x_{m,2} & \\dots  & x_{m,n}\n",
    "            \\end{bmatrix}$\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Number_of_Examples = Any\n",
    "Number_of_Features = Any\n",
    "Input_Matrix = NDArray[ ( Number_of_Examples, Number_of_Features ), Float64 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table width=100%>\n",
    "    <tr><td style=\"font-size:20px; text-align:left\">$Target\\ Matrix$</td></tr>\n",
    "    <tr>\n",
    "        <td style=\"font-size:24px; text-align:left\">\n",
    "            $Y = \\begin{bmatrix}\n",
    "                y_{1,1} & y_{1,2} & \\dots  & y_{1,k} \\\\\n",
    "                y_{2,1} & y_{2,2} & \\dots  & y_{2,k} \\\\\n",
    "                \\vdots  & \\vdots  & \\ddots & \\vdots \\\\\n",
    "                y_{m,1} & y_{m,2} & \\dots  & y_{m,k}\n",
    "            \\end{bmatrix}$\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Number_of_Targets = Any\n",
    "Target_Matrix = NDArray[ ( Number_of_Examples, Number_of_Targets ), Float64 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table width=100%>\n",
    "    <tr><td style=\"font-size:20px; text-align:left\">$Output\\ Matrix$</td></tr>\n",
    "    <tr>\n",
    "        <td style=\"font-size:24px; text-align:left\">\n",
    "            $\\hat{Y} = \\begin{bmatrix}\n",
    "                \\hat{y}_{1,1} & \\hat{y}_{1,2} & \\dots & \\hat{y}_{1,k} \\\\\n",
    "                \\hat{y}_{2,1} & \\hat{y}_{2,2} & \\dots & \\hat{y}_{2,k} \\\\\n",
    "                \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "                \\hat{y}_{m,1} & \\hat{y}_{m,2} & \\dots & \\hat{y}_{m,k}\n",
    "            \\end{bmatrix}$\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Output_Matrix = NDArray[ ( Number_of_Examples, Number_of_Targets ), Float64 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table width=100%>\n",
    "    <tr><td style=\"font-size:20px; text-align:left\">$Perceptron$</td></tr>\n",
    "    <tr>\n",
    "        <td style=\"font-size:24px; text-align:left\">\n",
    "            $P^{(ℓ)}_k = \\begin{bmatrix}\n",
    "                \\omega_{1,k}^{(ℓ)} \\\\\n",
    "                \\omega_{2,k}^{(ℓ)} \\\\\n",
    "                \\vdots \\\\ \n",
    "                \\omega_{j,k}^{(ℓ)}\n",
    "            \\end{bmatrix}$\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"font-size:20px; text-align:left\">\n",
    "            $b_k^{(ℓ)} = \\omega_{0,k}^{(ℓ)}$\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Number_of_Inputs = Any\n",
    "Bias, Weight = Float64, Float64\n",
    "Perceptron = NDArray[ ( Number_of_Inputs, 1 ), Weight ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table width=100%>\n",
    "    <tr><td style=\"font-size:20px; text-align:left\">$Layer$</td></tr>\n",
    "    <tr>\n",
    "        <td style=\"font-size:24px; text-align:left\">\n",
    "            $\\Omega^{(ℓ)} = \\begin{bmatrix}\n",
    "                P_1^{(ℓ)} & P_2^{(ℓ)} & \\dots & P_k^{(ℓ)}\n",
    "            \\end{bmatrix}$\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"font-size:20px; text-align:left\">\n",
    "            $\\beta^{(ℓ)} = \\begin{bmatrix}\n",
    "                b_1^{(ℓ)} & b_2^{(ℓ)} & \\dots & b_k^{(ℓ)}\n",
    "            \\end{bmatrix}$\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Number_of_Perceptrons = Any\n",
    "Layer_Weights = NDArray[ ( Number_of_Inputs, Number_of_Perceptrons ), Perceptron ]\n",
    "Layer_Biases = NDArray[ ( Number_of_Perceptrons, ), Bias ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table width=100%>\n",
    "    <tr><td style=\"font-size:20px; text-align:left\">$Network$</td></tr>\n",
    "    <tr>\n",
    "        <td style=\"font-size:24px; text-align:left\">\n",
    "            $N = \\begin{bmatrix}\n",
    "                \\begin{bmatrix} \\Omega^{(1)} & \\Omega^{(2)} & \\dots & \\Omega^{(ℓ)} \\end{bmatrix} & \n",
    "                \\begin{bmatrix} \\beta^{(1)} & \\beta^{(2)} & \\dots & \\beta^{(ℓ)} \\end{bmatrix}\n",
    "            \\end{bmatrix}$\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Network_Weights = List[ Layer_Weights ]\n",
    "Network_Biases = List[ Layer_Biases ]\n",
    "class Network( TypedDict ) :\n",
    "    weights : Network_Weights\n",
    "    biases  : Network_Biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size:32px; color:orange; font-family:cambria\"><i>Creating the Neural Network</i></h1>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table width=100%>\n",
    "    <tr><td style=\"font-size:20px; text-align:left\">$Initialization$</td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNeuralNetwork :\n",
    "    \n",
    "    def __init__( self, perceptrons_per_hidden_layer : List[ int ] = [] ) -> None :\n",
    "        self.score = 0.0\n",
    "        self.perlayer = perceptrons_per_hidden_layer\n",
    "        self.network : Network = { 'weights' : [], 'biases' : [] }\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table width=100%>\n",
    "    <tr><td style=\"font-size:20px; text-align:left\">$Initialize\\ Random\\ Weights\\ and\\ Biases$</td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNeuralNetwork( FeedForwardNeuralNetwork  ) :\n",
    "    \n",
    "    def initialize( self, X : Input_Matrix, Y : Target_Matrix ) -> None :\n",
    "        inputs = X.shape[ 1 ]\n",
    "        self.network = { 'weights' : [], 'biases' : [] }\n",
    "        for perceptrons in self.perlayer :\n",
    "            self.network[ 'weights' ].append( np.random.rand( inputs, perceptrons ) - 0.5 )\n",
    "            self.network[ 'biases' ].append( np.random.rand( perceptrons ) - 0.5 )\n",
    "            inputs = perceptrons\n",
    "        self.network[ 'weights' ].append( np.random.rand( inputs, Y.shape[ 1 ] ) - 0.5 )\n",
    "        self.network[ 'biases' ].append( np.random.rand( Y.shape[ 1 ] ) - 0.5 )\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table width=100%>\n",
    "    <tr>\n",
    "        <td style=\"font-size:20px; text-align:center\" colspan=\"2\" >$Activation\\ Function$</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"font-size:20px; text-align:center\">$Sigmoid$</td>\n",
    "        <td style=\"font-size:24px; text-align:left\">$\n",
    "        f(x) = \\frac{1}{1+e^{-x}}\n",
    "        $</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"font-size:20px; text-align:center\">$Derivative$</td>\n",
    "        <td style=\"font-size:24px; text-align:left\">$\n",
    "        f(x) = \\frac{e^{-x}}{(1+e^{-x})^{2}}\n",
    "        $</td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNeuralNetwork( FeedForwardNeuralNetwork  ) :\n",
    "    \n",
    "    def activation( self, x : NDArray[ Float64 ] ) -> NDArray[ Float64 ] :\n",
    "        return 1.0 / ( 1.0 + np.exp( -x ) )\n",
    "    \n",
    "    def derivative( self, x : NDArray[ Float64 ] ) -> NDArray[ Float64 ] :\n",
    "        return np.exp( -x ) / np.square( 1.0 + np.exp( -x ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table width=100%>\n",
    "    <tr>\n",
    "        <td style=\"font-size:20px; text-align:center\" colspan=\"2\" >$Cost\\ and\\ Gradient$</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"font-size:20px; text-align:center\">$Cost\\ Function$</td>\n",
    "        <td style=\"font-size:24px; text-align:left\">$\n",
    "        C(Y;\\hat{Y}) = \n",
    "        \\sum_{h=1}^{m}{\\sum_{i=1}^{k} (\\hat{y}_{h,i} - y_{h,i}})^{2} =\n",
    "        \\sum_{h=1}^{m}{\\sum_{i=1}^{k} (a^{(ℓ)}_{h,i} - y_{h,i}})^{2} \n",
    "        $</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"font-size:20px; text-align:center\">$Gradient$</td>\n",
    "        <td style=\"font-size:24px; text-align:left\">$\n",
    "        \\nabla^{(ℓ)}_{a}C = \\begin{bmatrix}\n",
    "                \\frac{\\partial C}{\\partial a^{(ℓ)}_{1,1}} &\n",
    "                \\frac{\\partial C}{\\partial a^{(ℓ)}_{1,2}} &\n",
    "                \\dots & \n",
    "                \\frac{\\partial C}{\\partial a^{(ℓ)}_{1,k}} \n",
    "                \\\\\n",
    "                \\frac{\\partial C}{\\partial a^{(ℓ)}_{2,1}} &\n",
    "                \\frac{\\partial C}{\\partial a^{(ℓ)}_{2,2}} &\n",
    "                \\dots & \n",
    "                \\frac{\\partial C}{\\partial a^{(ℓ)}_{2,k}}\n",
    "                \\\\\n",
    "                \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "                \\\\\n",
    "                \\frac{\\partial C}{\\partial a^{(ℓ)}_{m,1}} &\n",
    "                \\frac{\\partial C}{\\partial a^{(ℓ)}_{m,2}} &\n",
    "                \\dots & \n",
    "                \\frac{\\partial C}{\\partial a^{(ℓ)}_{m,k}}\n",
    "            \\end{bmatrix} = 2(\\hat{Y}-Y) \n",
    "        $</td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNeuralNetwork( FeedForwardNeuralNetwork  ) :\n",
    "    \n",
    "    def cost( self, A : Output_Matrix, Y : Target_Matrix ) -> Float64 :\n",
    "        return np.square( A - Y ).sum()\n",
    "    \n",
    "    def grad( self, A : Output_Matrix, Y : Target_Matrix ) -> NDArray[ Float64 ] :\n",
    "        return 2*( A - Y )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table width=100%>\n",
    "    <tr>\n",
    "        <td style=\"font-size:20px; text-align:center\" colspan=\"2\" >$Forward\\ Propagation$</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"font-size:20px\">$Output\\ and\\ Hidden\\ Layer$</td>\n",
    "        <td style=\"font-size:24px\">\n",
    "            $L^{(ℓ)}=f*(L^{(ℓ-1)}\\Omega^{(ℓ)} + \\beta^{(ℓ)})$\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"font-size:20px\">$Input\\ Layer$</td>\n",
    "        <td style=\"font-size:24px; text-align:left\">$L^{(0)} = X$</td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNeuralNetwork( FeedForwardNeuralNetwork  ) :\n",
    "    \n",
    "    def forwardpropagation( self, L : Input_Matrix ) -> Output_Matrix :\n",
    "        for w, b in zip( self.network[ 'weights' ], self.network[ 'biases' ] ) :\n",
    "            L = self.activation( np.matmul( L, w ) + b ) \n",
    "        return L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table width=100%>\n",
    "    <tr>\n",
    "        <td style=\"font-size:20px; text-align:center\" colspan=\"2\" >$Backpropagation$</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"font-size:20px\" rowspan=\"2\">$Containers$</td>\n",
    "        <td style=\"font-size:24px; text-align:left\">$Z[ℓ] = L^{(ℓ-1)}\\Omega^{(ℓ)}+\\beta^{(ℓ)}$</td>\n",
    "    </tr>\n",
    "    <tr><td style=\"font-size:24px\">$A[ℓ] = L^{(ℓ)}$</td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNeuralNetwork( FeedForwardNeuralNetwork  ) :\n",
    "    \n",
    "    def __forwardpropagation( self, X : Input_Matrix, A : List, Z : List ) -> Output_Matrix :\n",
    "        A.append( X ) # len( A ) = len( Z ) + 1\n",
    "        for w, b in zip( self.network[ 'weights' ], self.network[ 'biases' ] ) :\n",
    "            # weighted input to layer\n",
    "            Z.append( np.matmul( A[ -1 ], w ) + b )\n",
    "            # output of layer\n",
    "            A.append( self.activation( Z[ -1 ] ) )\n",
    "        return A[ -1 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table width=100%>\n",
    "    <tr>\n",
    "        <td style=\"font-size:20px; text-align:center\" colspan=\"2\" >$Backpropagation$</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"font-size:20px\">$Initialization$</td>\n",
    "        <td style=\"font-size:24px; text-align:left\">\n",
    "            $\n",
    "            \\nabla^{(ℓ)}_{z}C=\n",
    "            (f'*Z[ℓ])*\\nabla^{(ℓ)}_{a}C\n",
    "            $\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"font-size:20px\" rowspan=\"3\">$Output\\ and\\ Hidden\\ Layers$</td>\n",
    "        <td style=\"font-size:24px\">$\\nabla^{(ℓ)}_{\\Omega}C=A[ℓ-1]^{T}\\nabla^{(ℓ)}_{z}C$</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"font-size:24px\">$\\nabla^{(ℓ)}_{\\beta}C=\\pmb{1}_{1xm}\\nabla^{(ℓ)}_{z}C$</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"font-size:24px\">$\\nabla^{(ℓ-1)}_{z}C=(f'*Z[ℓ-1])*\\nabla^{(ℓ)}_{z}C(\\Omega^{(ℓ)})^{T}$</td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNeuralNetwork( FeedForwardNeuralNetwork  ) :\n",
    "    \n",
    "    def backpropagate( self, grad_z : NDArray[ Float64 ], A : List, Z : List, layer_index : int ) -> Tuple :\n",
    "        # gradient with respect to the weights of the layer\n",
    "        grad_w = np.matmul( A[ layer_index ].T, grad_z ) # len( A ) = len( Z ) + 1\n",
    "        # gradient with respect to the biases of the layer\n",
    "        grad_b = grad_z.sum( axis = 0 )\n",
    "        # gradient with respect to the weighted input of the layer\n",
    "        if layer_index > 0 : # there is no weighted input for layer 0\n",
    "            grad_z = self.derivative( Z[ layer_index - 1 ] ) *\\\n",
    "                     np.matmul( grad_z, self.network[ 'weights' ][ layer_index ].T )\n",
    "        return grad_z, grad_w, grad_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table width=100%>\n",
    "    <tr>\n",
    "        <td style=\"font-size:20px; text-align:center\">$Gradient Descent$</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"font-size:24px; text-align:left\">\n",
    "            $\\Omega^{(ℓ)} \\rightarrow \\Omega^{(ℓ)} - \\frac{r}{m}\\nabla^{(ℓ)}_{\\Omega}C$\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"font-size:24px; text-align:left\">\n",
    "            $\\beta^{(ℓ)} \\rightarrow \\beta^{(ℓ)} - \\frac{r}{m}\\nabla^{(ℓ)}_{\\beta}C$\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNeuralNetwork( FeedForwardNeuralNetwork  ) :\n",
    "    \n",
    "    def train( self, X : Input_Matrix, Y : Target_Matrix, \n",
    "               learning_rate = 1.0, convergence = 0.01, \n",
    "               batch_size = 10, max_epoch = 500, max_time = 60 ) -> None :\n",
    "        ''' Stochastic Gradient Descent '''\n",
    "        epoch = 1\n",
    "        start = time()\n",
    "        totgrad = np.inf\n",
    "        self.initialize( X, Y )\n",
    "        output_layer_index = len( self.network[ 'weights' ] ) - 1\n",
    "        while np.sqrt( totgrad ) > convergence :\n",
    "            totgrad = 0\n",
    "            shuffle = np.random.permutation( len( X ) )\n",
    "            X, Y = X[ shuffle ], Y[ shuffle ]\n",
    "            for batch_x, batch_y in zip( \n",
    "                np.array_split( X, len( X ) // batch_size ), \n",
    "                np.array_split( Y, len( Y ) // batch_size ) \n",
    "                ) :\n",
    "                A, Z = [], []\n",
    "                output = self.__forwardpropagation( batch_x, A, Z )\n",
    "                # gradient with respect to the output layer\n",
    "                grad_a = self.grad( output, batch_y )\n",
    "                totgrad += np.linalg.norm( grad_a )**2\n",
    "                # gradient with respect to the weighted input of the output layer\n",
    "                grad_z = self.derivative( Z[ -1 ] ) * grad_a\n",
    "                for layer_index in range( output_layer_index, -1, -1 ) :\n",
    "                    grad_z, grad_w, grad_b = self.backpropagate( grad_z, A, Z, layer_index )\n",
    "                    # updating the weights and biases of layer\n",
    "                    self.network[ 'weights' ][ layer_index ] -= learning_rate * grad_w / len( batch_x )\n",
    "                    self.network[ 'biases' ][ layer_index ] -= learning_rate * grad_b / len( batch_x )\n",
    "            epoch += 1\n",
    "            if time() - start > max_time :\n",
    "                print( 'Maximum runtime encountered.' )\n",
    "                break\n",
    "            if epoch > max_epoch :\n",
    "                print( 'Maximum epoch encountered.' )\n",
    "                break\n",
    "        self.score = self.cost( self.forwardpropagation( X ), Y )\n",
    "        return"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
